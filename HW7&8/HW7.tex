\documentclass{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{parskip}
\usepackage{graphicx}
\usepackage{hyperref}
% \usepackage{unicode-math}
\usepackage{dsfont}
% \usepackage{bbm}
% Margins
\usepackage[top=2.5cm, left=3cm, right=3cm, bottom=4.0cm]{geometry}
% Colour table cells
\usepackage[table]{xcolor}

% Get larger line spacing in table
\newcommand{\tablespace}{\\[1.25mm]}
\newcommand\Tstrut{\rule{0pt}{2.6ex}}         % = `top' strut
\newcommand\tstrut{\rule{0pt}{2.0ex}}         % = `top' strut
\newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}}   % = `bottom' strut
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\Var}{\operatorname{Var}}
\DeclareMathOperator{\Cov}{\operatorname{Cov}}
\newcommand{\vu}{\boldsymbol{u}}
%%%%%%%%%%%%%%%%%
%     Title     %
%%%%%%%%%%%%%%%%%
\title{Exercise 7 \& 8 \\ Probability Theory 2020 Autumn}
\author{Hanmo Chen \\ 2020214276}
\date{\today}

\begin{document}
\maketitle

\section{Problem 1}

Denote the entries in $U$ as $u_{ij}$ and entries in $Y$ as $Y_{ij}$,so 

\begin{equation}
    Y_{ij} = \sum_{r,s} u_{ri} u_{sj} X_{ij} 
\end{equation}

Also we have,

\begin{equation}
    \Cov(X_{ij},X_{mn}) = \left\{\begin{aligned}
        &2,\quad i=j=m=n \\
        &1,\quad (i,j) = (m,n) \text{ or } (i,j) = (n,m), i \neq j \\
        &0,\quad \text{otherwise}
    \end{aligned}\right.
\end{equation}

Thus

\begin{equation}
    \begin{aligned}
        \Cov(Y_{ij},Y_{mn}) & = \Cov\left(\sum_{r,s} u_{ri} u_{sj} X_{ij}, \sum_{p,q} u_{pm} u_{qn} X_{mn} \right) \\
        & = 2\sum_{r=1}^n u_{ri}u_{rj} u_{rm}u_{rn} + \sum_{r\neq s} u_{ri} u_{sj} u_{rm} u_{sn} + \sum_{r\neq s} u_{ri} u_{sj} u_{rn} u_{sm} \\
        & = \sum_{r,s}\left[u_{ri} u_{sj} u_{rm} u_{sn}+u_{ri} u_{sj} u_{rn} u_{sm} \right] \\
        & = \left(\sum_{r} u_{ri} u_{rm} \right) \left(\sum_{s} u_{sj} u_{rn} \right) + \left(\sum_{r} u_{ri} u_{rn} \right) \left(\sum_{s} u_{sj} u_{sm} \right) 
    \end{aligned}
\end{equation}

Denote the column vectors in $U$ as $\vu_i,i=1,2,3,\cdots,N$, so $\vu_i\cdot \vu_j = \sum_{r} u_{ri} u_{rj} = \delta_{ij}$. 

\begin{equation}
    \Cov(Y_{ij},Y_{mn}) = \delta_{im} \delta_{jn} +  \delta_{jm} \delta_{in} = \left\{ \begin{aligned}
        &2,\quad i=j=m=n \\
        &1,\quad (i,j) = (m,n) \text{ or } (i,j) = (n,m), i \neq j \\
        &0,\quad \text{otherwise}
    \end{aligned}\right.
\end{equation}

Because $X_{ij}, j \geqslant i $ are independent Gaussian variables, so the joint distribution of $Y_{ij}$ is joint Gaussian distribution, which means,

\begin{equation}
    \Cov(Y_{ij},Y_{mn}) = 0 \Longleftrightarrow Y_{ij},Y_{mn} \text{ are independent}
\end{equation}

So all the entries on and above the diagonal of $Y$ are independent, and $Y_{ii} \sim N(0,2), i =1,2,3,\cdots,N$ and $Y_{ij} \sim N(0,1), 1\leqslant i < j \leqslant n$. (It is easy to see that $\E[Y_{ij}] = 0$)

\section{Problem 2}

Notice that \begin{enumerate}
    \item If $X_n = 1$, then $X_{n+1},X_{n+2},\cdots$ are independent of $X_{1},X_2,\cdots,X_{n}$  
    \item There is at least one $1$ in any five-in-a-row $X_i$s as $\{X_n,X_{n+1},\cdots,X_{n+4} \} $
\end{enumerate}

So we can split $X_1,X_2,\cdot,X_n$ into a series of epsisodes, each epsisode $L_j = [0,\cdots,0,1]$ is consisted of $n$ zeros ($n$ can be $0,1,2,3,4$) and $1$ one. And $L_j, j =1,2,\cdots,m$ are independent. (For the last epsisode, if it is ended with $0$, we can append 1 to its end and let $n = n+1$.) Denote the length of each epsisode as $l_j$, so $\sum_{j=1}^m l_j = n$.

Consider the distribution of $l_j$, it can only take values in $1,2,3,4,5$, \begin{itemize}
    \item $P(l_j = 1) = P(X_1=1)  = 0.2$
    \item $P(l_j = 2) = P(X_1=0,X_2 = 1)= 0.16$ 
    \item $P(l_j = 3) = P(X_1=0,X_2 = 0,X_3 = 1)= 0.128$
    \item $P(l_j = 4) = P(X_1=0,X_2 = 0,X_3 = 0,X_4=1)= 0.1024$
    \item $P(l_j = 5) = P(X_1=0,X_2 = 0,X_3 = 0,X_4=0) = 0.4096$
\end{itemize}

So 

\begin{equation}
    \lim_{n\to \infty} \frac{S_n}{ n }= \lim_{m\to \infty} \frac {m}{l_1 + l_2 + \cdots + l_m  } = \lim_{m\to \infty} \frac{1}{ \frac{1}{m} \sum_{j=1}^m l_j}
\end{equation}


According to Strong Law of Large Numbers,

\begin{equation}
    \frac{1}{m} \sum_{j=1}^m l_j \overset{a.s}{\longrightarrow} E[l_j] = 3.3616 
\end{equation}

So \begin{equation}
    \lim_{n\to \infty} \frac{S_n}{ n } \overset{a.s}{\longrightarrow} \frac{1}{3.3616}
\end{equation}


\section{Problem 3}

\subsection{(i)}
Suppose the corresponding $k$ of $X_n$ is $k_n$, i.e. $\sum_{i=1}^{k_n} Y_i = X_n+n$.
If $X_n \geqslant 1$, $\sum_{i=1}^{k_n}  Y_i \geqslant n+1$, so $k_{n+1} = k_n,X_{n+1} = X_n -1$.
If $X_n = 0$, $\sum_{i=1}^{k_n} Y_i= n,  \sum_{i=1}^{k_n+1} Y_i= n+Y_{n+1} \geqslant n+1$, so $k_{n+1} = k_n,X_{n+1} = Y_{n+1}-1$.
 
So given $X_{n}$, $X_{n+1}$ is independent of $X_{n-1},\cdots,X_1$. $\{X_n\}_{n=1}^{\infty}$ forms a Markov Chain. And the transition probability is,

\begin{equation}
    P(X_{n+1} = i | X_{n} = 0) = p_{i+1}, i =0,1,\cdots
\end{equation}

\begin{equation}
    P(X_{n+1} = i | X_{n} = j,j\geqslant 1) =  \left\{ \begin{aligned}
        &1, \quad i=j-1 \\
        &0, \quad\text{otherwise}
    \end{aligned}
        \right.
\end{equation}

\subsection{(ii)}

Notice that $f(n) = P(X_n =0)$, so $\lim\limits_{n\to\infty} f(n) = \lim\limits_{n\to\infty} P(X_n =0)$. If we want $\lim_{n \to \infty} f(n)$ exists, the Markov chain must be irreducible, aperiodic and positive recurrent.

It is irreducible obviously. Consider the support set $\mathcal{Y}=\{i:p_i >0 \} $ of $Y$, if $\inf \mathcal{Y} = N < \infty$, the state space $\mathcal{S}$ of the Markov Chain is finite $\{0,1,\cdots,N\}$. Obviously $N$ can be reached from $0$.  And because $N-1,N-2,\cdots,0$ can be reached from $N$, so it is irreducible. If $\inf \mathcal{Y} =  \infty$, for any state $n$, there exists a state $m>n$, and  $m$ can be reached from $0$, so $n$ can be reached from $0$. In that case, the Markov chain is also irreducible.

For it to be aperiodic, if it comes from 0 to $i$, it will return to 0 in $i$ steps. So if $\mathcal{Y}=\{i:p_i >0 \} $ is like $\{2,4,\cdots,2k,\cdots\}$ or $\{3,6,9,\cdots,3k,\cdots,\}$, for certian steps it will not arrive at $0$. So the Markov chain is aperiodic if and only if $\operatorname{gcd}(\mathcal{Y} ) = 1$

And it is positive recurrent if and only if $\E[T_0] < \infty$. It is easy to see that $P(T_0 = i+1) = p_i, i\geqslant 1$, so 

\begin{equation}
    \E[T_0] = \sum_{i=1}^{\infty} ip_i = \E[Y_1] 
\end{equation}

So the necessary and sufï¬cient condition for $\lim\limits_{n\to\infty} f(n)$ to exist is $\operatorname{gcd}(\{i+1: p_i >0\}) = 1$ and $\sum_{i=1}^{\infty} ip_i  < \infty$

\subsection{(iii)}

The limis equals to the steady-state probability, 

\begin{equation}
    \pi_0 = \lim_{n\to\infty} f(n) = \frac{1}{\E[T_0]} = \frac{1}{\mu}
\end{equation}

\section{Problem 4}


% Denote $S_k = \sum_{i=0}^k Y_i$ and $S_0 = 0$

% \begin{equation}
%     f(n) = \sum_{k=0}^{\infty} P(S_k = n) 
% \end{equation}

% Notice that $P(S_n = k) = 0$ for $n>k$,

% \begin{equation}
%     \begin{aligned}
%         f(n) & = \sum_{k=0}^{n} P(S_k = n) = \sum_{k=1}^{n} \sum_{j=0}^{n-1} P(S_{k-1} = j,Y_k = n-j) \\
%         & = \sum_{j=0}^{n-1} p_{n-j}\sum_{k=1}^{n}  P(S_{k-1} = j) = \sum_{j=0}^{n-1} p_{n-j} f(j)
%     \end{aligned}
% \end{equation}

% Denote $f_n= f(n)$ and $f_0 = 0$ and the genetating function of $f_n$ and $p_n$ is

% \begin{equation}
%     F(z) = \sum_{i=0}^{\infty} f_i z^i, \quad P(z) = \sum_{i=1}^{\infty} p_i z^i
% \end{equation}

% We have

% \begin{equation}
%     F(z)-1 = \sum_{n=1}^{\infty} f_n z^n = \sum_{n=1}^{\infty} \sum_{j=0}^{n-1} p_{n-j} f_j z^{n} = (\sum_{j=0}^{\infty} f_{j}z^j ) (\sum_{i=1}^{\infty} p_i z^i) = F(z) U(z)
% \end{equation}

% So 

% \begin{equation}
%     F(z) = \frac{1}{1-U(z)}
% \end{equation}


\end{document} 
