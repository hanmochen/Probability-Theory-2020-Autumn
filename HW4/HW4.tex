\documentclass{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{parskip}
\usepackage{graphicx}
\usepackage{hyperref}
% Margins
\usepackage[top=2.5cm, left=3cm, right=3cm, bottom=4.0cm]{geometry}
% Colour table cells
\usepackage[table]{xcolor}

% Get larger line spacing in table
\newcommand{\tablespace}{\\[1.25mm]}
\newcommand\Tstrut{\rule{0pt}{2.6ex}}         % = `top' strut
\newcommand\tstrut{\rule{0pt}{2.0ex}}         % = `top' strut
\newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}}   % = `bottom' strut

%%%%%%%%%%%%%%%%%
%     Title     %
%%%%%%%%%%%%%%%%%
\title{Exercise 4 \\ Probability Theory 2020 Autumn}
\author{Hanmo Chen \\ Student ID 2020214276}
\date{\today}

\begin{document}
\maketitle
\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%
%   Problem 1   %
%%%%%%%%%%%%%%%%%
\section{Problem 1}
\subsection{(i)}

Claim that $M_X(s) = \infty$ for all $s\neq 0$. The proof is as below.

\begin{equation}
    M_X(s) = \mathbb{E}[e^{sx}] = \int_{-\infty}^{\infty} \frac{1}{\pi} \frac{t}{t^2+x^2} e^{sx} dx = \int_{-\infty}^{\infty} \frac{1}{\pi} \frac{t}{t^2+x^2} e^{-sx} dx
\end{equation}

So $M_X(s) = M_X(-s)$, we just need to prove $M_X(s) = \infty$ for $s>0$. 

Notice that for $x>0$, $e^x > \frac {x^3}{6}$.

\begin{equation}
    \begin{aligned}
        M_X(s) & = \int_{-\infty}^{\infty} \frac{1}{\pi} \frac{t}{t^2+x^2} e^{sx} dx > \int_{0}^{\infty} \frac{1}{\pi} \frac{t}{t^2+x^2} e^{sx} dx \\
        & > \int_{0}^{\infty} \frac{1}{6\pi} \frac{ts^3 x^3}{t^2+x^2} dx =  \int_{0}^{\infty} \frac{t^3s^3}{6\pi} \frac{x^3}{1+x^2} dx \\
        & = \frac{t^3s^3}{12\pi} (x^2-\ln(1+x^2)) \big|_{0}^{\infty} = \infty
    \end{aligned}
\end{equation}

So $M_X(s) = \infty$ for all $s\neq 0$. And it is trivial that $M_X(0) = 1$.


\begin{equation}
    M_X(s) = \left\{  \begin{aligned}
        &1,\quad s =0\\
        &\infty, \quad s\neq 0
    \end{aligned}
        \right.
\end{equation}

\subsection{(ii)}

Yes. An example is given as the symmetrized lognormal distribution.

For example, $Z \sim N(0,1)$ and $Y = e^Z \sim \text{lognormal}(0,1)$ and define $X$ as 

\begin{equation}
    X = \left\{  \begin{aligned}
        &Y,\quad \text{with probability } \frac{1}{2} \\
        &-Y, \quad \text{with probability } \frac{1}{2}
    \end{aligned}\right.
\end{equation}

It is easy to verify that 

\begin{equation}
    f_Y(y) = f_Z(\ln y ) \frac {1}{y} = \frac{1}{\sqrt{2\pi} y} e^{-\frac{1}{2} (\ln y)^2}
\end{equation}

And the $n$-th moment of $Y$ is 

\begin{equation}
    \begin{aligned}
        \mathbb{E}[Y^n] & = \int_{0}^{\infty}  \frac{1}{\sqrt{2\pi} y} e^{-\frac{1}{2} (\ln y)^2} * y^n dy = \int_{0}^{\infty} \frac{1}{\sqrt{2\pi} y} \exp \left( -\frac{1}{2 }(\ln y)^2 + n\ln y\right) dy \\
        & = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} z^2 + \frac{n^2}{2}\right) dz \quad \left(z=\ln y - n\right) \\
        & = e^{\frac{n^2}{2}}
    \end{aligned}
\end{equation}

And 

\begin{equation}
    f_X(x) = \frac{1}{2\sqrt{2\pi} |x|} e^{-\frac{1}{2} (\ln |x|)^2}
\end{equation}


Then we will show that all the moments of $X$ are finite. For all odd integers $k$,

\begin{equation}
    \mathbb{E}[X^k] = \mathbb{E}[X^k_+] - \mathbb{E}[X^k_-] = \frac{1}{2} \mathbb{E}[Y^k] -  \frac{1}{2} \mathbb{E}[Y^k] = 0
\end{equation}

For all even integers $k$,

\begin{equation}
    \mathbb{E}[X^k] = \frac{1}{2} \mathbb{E}[Y^k] +  \frac{1}{2} \mathbb{E}[Y^k] = e^{\frac{n^2}{2}}
\end{equation}

So $\mathbb{E}[X^k] < \infty$ for all integers $k\geqslant 1$. But for the moment generating function $M_X(s)$, assuming $s>0$,

\begin{equation}
    \begin{aligned}
        M_X(s) & = \mathbb{E}[e^{sx}] = \int_{-\infty}^{\infty}  \frac{1}{2\sqrt{2\pi} |x|} e^{-\frac{1}{2} (\ln |x|)^2} e^{sx} dx > \int_{0}^{\infty}  \frac{1}{2\sqrt{2\pi} x} e^{-\frac{1}{2} (\ln x)^2} e^{sx} dx  \\
        & = \int_{-\infty}^{\infty}  \frac{1}{2\sqrt{2\pi} } e^{-\frac{1}{2} z^2} e^{se^z} dz > \int_{0}^{\infty} \frac{1}{2\sqrt{2\pi} } e^{\frac{s}{6} z^3-\frac{1}{2} z^2} \\
        & >  \int_{\frac{6}{s}}^{\infty} \frac{1}{2\sqrt{2\pi} } e^{\frac{1}{2} z^2} = \infty \quad (\frac{s}{6} z^3 > z^2 \text{ when } z>\frac{6}{s}) 
    \end{aligned}
\end{equation}

$M_X(s) = \infty$ for $s>0$ and $M_X(s) = M_X(-s)$, so  $M_X(s) = \infty$ for $s\neq 0$

\section{Problem 2}

\subsection{(a)}

Notice that the conclusion in (a) is a weaker version of the conclusion from (b) because $X^k \leqslant X^k e^{sX}$ when $s>0$, we will just prove (b).

\subsection{(b)}

If $s<0$, $X^k e^{sX} \leqslant X^k \leqslant X^k e^{s'X},s'>0$, so we will just prove the case when $s>0$.

Because $X$ is nonnegative,

\begin{equation}
    e^{ax} = e^{(a-s)x}  > \frac{(a-s)^k x^k}{k!}e^{sx}
\end{equation}

So $x^ke^{sx} < e^{ax} \frac {k!}{(a-s)^k}$,

\begin{equation}
    \mathbb{E}[X^ke^{sX}] \leqslant \mathbb{E}[\frac {k!}{(a-s)^k} e^{aX}] = \frac {k!}{(a-s)^k} M_X(a) < \infty
\end{equation}

\subsection{(c)}

The inequality holds true only when $h>0$. The following proof is given under the condition $h>0$.

It's obvious when $X=0$. And for $X>0$, it is equivalent to $\frac {e^{hX}-1}{hX} \leqslant e^{hX}$, denote $b$ as $hX$ and $g(x)$ as $e^X$, using Lagrange's mean value theorm,

\begin{equation}
    \frac{e^{hX}-1}{hX} = \frac{g(b)-g(0)}{b-0} = g'(c) \leqslant g'(b) = e^{hX}, c \in (0,b)
\end{equation}

\subsection{(d)}

Using L'Hospital's Rule, $X = \lim\limits_{h\to 0} \frac{e^{hX}-1}{h}$. 
Suppose $\{h_n\}$ is a sequence of nonnegative numbers and $\lim\limits_{n\to \infty} h_n = 0$, so $h_n<\frac{a}{2}$ for sufficiently large $n$. And define $X_n = \frac {e^{h_n X}-1}{h_n}$.

Using the conclusion from (c)ï¼Œ

\begin{equation}
    0<X_n\leqslant  Xe^{h_nX} < Xe^{\frac{a}{2}X}
\end{equation}  

And $\mathbb{E}[Xe^{\frac{a}{2}X}] < \infty$, so according to the Dominant Convergence Theorm,

\begin{equation}
    \mathbb{E}[X] = \mathbb{E}[\lim_{h\downarrow 0 } \frac{e^{hX}-1}{h}] = \lim_{h\downarrow 0 } \mathbb{E}[ \frac{e^{hX}-1}{h}] = \lim_{h\downarrow 0 } \frac{\mathbb{E}[e^{hX}]-1}{h}
\end{equation}

\section{Problem 3}

\textbf{Yes.}

To prove this, we define a series of random variables by replacing $X_i$ with $Y_i$ once a time. 

Define $Z_0 =X =  X_1+X_2+\cdots+X_n, Z_1 = Y_1 + X_2 + \cdots +X_n, Z_j= \sum_{i=1}^{j} Y_j + \sum_{i=J+1}^{n} X_i, Z_n = Y = Y_1+ Y_2 +\cdots+Y_n$

We want to prove $Z_{j}$ \textit{stochastically dominates} $Z_{j-1}$, that is,

$\forall k = 1,2,\cdots,n$, $P(Z_{j-1}\geqslant k) \geqslant P(Z_j\geqslant k)$

Notice that the only difference between $Z_{j-1}$ and $Z_j$ is the $j$-th item, $X_{j}$ or $Y_{j}$. 

Denote the sum of the left $n-1$ items as $Z'$, and $Z_{j-1} = Z'+X_{j},Z_{j}=Z'+Y_{j}$, also denote $P(X_{i} =1) = p_i \geqslant P(Y_{i} =1) = q_i$.

\begin{equation}
\begin{aligned}
    P(Z_{j-1}\geqslant k) &=  P(Z'+X_j \geqslant k) \\ &= P(Z'\geqslant k) + P(Z' = k-1,X_j=1) \\ &= P(Z'\geqslant k)+ P(Z' = k-1)p_j 
\end{aligned}
\end{equation}

In the same way, $ P(Z_{j}\geqslant k) = P(Z'\geqslant k)+ P(Z' = k-1)q_j $.

Since $p_j \geqslant q_j$, we prove that $\forall k = 1,2,\cdots,n,\quad P(Z_{j-1}\geqslant k) \geqslant P(Z_j\geqslant k)$


Therefore, $\forall k = 1,2,\cdots,n$

\begin{equation}
    P(X\geqslant k) = P(Z_{0}\geqslant k) \geqslant P(Z_{1}\geqslant k) \geqslant \cdots \geqslant P(Z_{n}\geqslant k) = P(Y \geqslant k)
\end{equation}

\section{Problem 4}

\subsection{(a)}

Suppose $V \sim \text{Exp}(\lambda)$, because $U,V$ are independent, $\operatorname{E}\left[\frac{V^2}{1+U}\right]= \operatorname{E}[V^2] \operatorname{E}\left[\frac{1}{1+U}\right]$.

\begin{equation}
    \operatorname{E}[V^2]  = \int_{0}^{\infty} \lambda x^2 e^{-\lambda x} dx = \int_{0}^{\infty} x^2 e^{- x} dx = -\frac{1}{\lambda^2} e^{-x} (2 + 2 x + x^2) \big|_{0}^{\infty} = \frac{2}{\lambda^2}
\end{equation}

\begin{equation}
    \operatorname{E}\left[\frac{1}{1+U}\right] = \int_{0}^{1} \frac {1}{1+x} dx = \ln 2
\end{equation}

So,

\begin{equation}
    \operatorname{E}\left[\frac{V^2}{1+U}\right]= \operatorname{E}[V^2] \operatorname{E}\left[\frac{1}{1+U}\right] = \frac{2\ln 2}{\lambda^2}
\end{equation}

\subsection{(b)}

\begin{equation}
    \begin{aligned}
        P(U\leqslant V) & = \int_{0}^1 P(V\geqslant u) f_U(u)du \\
        & = \int_{0}^1 \int_{u}^{\infty} \lambda  e^{-\lambda v} dv du \\
        & =  \int_{0}^1 e^{-\lambda u}du \\
        & =  \frac 1 {\lambda} (1-e^{-\lambda})
    \end{aligned}
\end{equation}

\subsection{(c)}

Because $U =\sqrt{Y}, V = \frac{Z}{\sqrt{Y}}$ 

\begin{equation}
    f_{Y,Z}(y, z)=f_{U,V}(u(y,z), v(y,z))\left|\frac{\partial(u,v)}{\partial(y,z)}\right|
\end{equation}

And the Jacobian matrix is,

\begin{equation}
    \begin{aligned}
        \left|J\right| = \left|\frac{\partial(u,v)}{\partial(y,z)}\right| = \left|\begin{matrix}
            \frac{1}{2\sqrt{Y}}&0\\-\frac{Z}{2Y\sqrt{Y}}&\frac{1}{\sqrt{Y}} 
        \end{matrix}\right| = \frac{1}{2Y}
    \end{aligned}
\end{equation}

So the joint pdf of $Y,Z$ is 

\begin{equation}
    \begin{aligned}
        f_{Y,Z}(y, z) = 1\times \lambda e^{-\lambda \frac z {\sqrt{y}}} \times \frac {1}{2y}
    \end{aligned}
\end{equation}

The support set of $Y,Z$ is $[0,1]\times [0,\infty)$.

\begin{equation}
    f_{Y,Z}(y, z) = \left\{\begin{aligned}
        &\frac {\lambda }{2y} e^{-\lambda \frac z {\sqrt{y}}}, \quad (y,z) \in [0,1]\times [0,\infty) \\
        &0,\quad \text{else}
    \end{aligned}\right.
\end{equation}

\section{Problem 5}

First $X_2+X_3$ follows the Gamma distribution $\Gamma(2,\frac{1}{\lambda})$ with $\frac{1}{\lambda}$ as the scale parameter, to derive this, let $Z = X_2+X_3$,

\begin{equation}
    \begin{aligned} f_{Z}(z) &=\int_{-\infty}^{\infty} f_{X_{2}}\left(x\right) f_{X_{3}}\left(z-x\right) d x 
    \\ &=\int_{0}^{z} \lambda e^{-\lambda x} \lambda  e^{-\lambda\left(z-x\right)} d x \\
    & =\int_{0}^{z} \lambda^2 e^{-\lambda z}  d x \\
& = \lambda^2 z e^{-\lambda z}\end{aligned}
\end{equation}


So

\begin{equation}
    \begin{aligned}
        P(X_1 > X_2+X_3) & = P(X_1 > Z) = \int_{0}^{\infty} f_{X_1}(x) P(Z<x)dx \\
        & = \int_{0}^{\infty}\left(\int_{0}^x \lambda^2 z e^{-\lambda z} dz \right) \lambda e^{-\lambda x} dx \\
        & =  \int_{0}^{\infty} \left(1-(1+\lambda x)e^{-\lambda x}\right) \lambda e^{-\lambda x} dx \\
        & = 1 - \int_{0}^{\infty} \lambda e^{-2\lambda x}dx - \int_{0}^{\infty} \lambda^2 x e^{-2\lambda x}dx \\
        & = 1 - \frac 1 2 - \frac{1}{4} = \frac 1 4 
    \end{aligned}
\end{equation}

\section{Problem 6}

The eigenvalues $Y_1,Y_2$ are the roots of the following equation,

\begin{equation}\label{eq1}
    (\lambda-X_1)(\lambda-X_2) = X_3^2
\end{equation}

\textbf{Part 1}

First, let $U=\frac {X_1+X_2} 2,V = \frac {X_1-X_2} 2$, we prove that $U,V \overset{i.i.d.}{\sim} N(0,1)$.

\begin{equation}
    \begin{aligned}
        f_{U,V}(u, v) & =f_{X_1,X_2}(x_1, x_2)\left|\frac{\partial(x_1, x_2)}{\partial(u, v)}\right| \\
        & = \frac {1} {4\pi } e^{-\frac {x_1^2+x_2^2}{4}} \left\|\begin{matrix}
            1  & 1 \\ 
            1 &  -1
        \end{matrix}\right\| \\
        &  = \frac {1}{2\pi} e^{-\frac{u^2 + v^2}{2}} \\
        & = \frac{1}{\sqrt{2\pi}} e^{-\frac{u^2}{2}} \times \frac{1}{\sqrt{2\pi}} e^{-\frac{v^2}{2}}
    \end{aligned}
\end{equation}

So $U,V$ are independent and follow the standard Gaussian distribution.

~\\

\textbf{Part 2}

By solving the equation \ref{eq1} we have 

\begin{equation}
    \left\{
    \begin{aligned}
        &Y_1 +Y_2 = X_1+X_2 = 2U \\
        &|Y_1 -Y_2| = 2\sqrt{\left(\frac{X_1-X_2}{2}\right)^2 + X_3^2} = 2\sqrt{V^2 + X_3^2}
    \end{aligned}
    \right.
\end{equation}

Denote $Z_1= \frac{Y_1+Y_2}{2}$, $Z_2 = \frac{|Y_1 -Y_2|}{2}$, so $Z_1 = U \sim N(0,1),Z_2 = \sqrt{V^2+X_3^2}$.

Because $X_1,X_2,X_3$ are independent, $U,V,X_3$ are also independent, $Z_1$ and $Z_2$ are independent. Because $V$ and $X_3$ are standard Gaussian random variables $,Z_2 = \sqrt{V^2+X_3^2} \sim \chi_{2}$ (also known as the Rayleigh distribution)

To derive $f_{Z_2}(z)$,

\begin{equation}
    \begin{aligned}
        F_{Z_2}(z) & = P(Z_2\leqslant z)  = P(X_3^2+V^2\leqslant z^2) = \iint_{x^2+y^2\leqslant z^2 } \frac{1}{2\pi} e^{-\frac{x^2+y^2}{2}} dxdy 
        \\ & = \frac 1{2\pi} \int_{0}^{2\pi} \int_{0}^{z} re^{-\frac{r^2}{2}} drd\theta \\
        & = \int_{0}^{z} re^{-\frac{r^2}{2}} dr \\
        & = 1- e^{-\frac {z^2}{2}}
    \end{aligned}
\end{equation}

And $f_{Z_2}(z) = F'_{Z_2}(z) = z e^{-\frac {z^2}{2}}, z\geqslant 0$

~\\

\textbf{Part 3}

Assume $Y_1 \geqslant Y_2$, 

\begin{equation}
    \begin{aligned}
        f_{Y_1,Y_2}(y_1, y_2) & =f_{Z_1,Z_2}(z_1, z_2)\left|\frac{\partial(z_1, z_2)}{\partial(y_1, y_2)}\right| \\
        & = \frac {1}{\sqrt{2\pi}} e^{-\frac{z_1^2}{2}} z_2 e^{-\frac {z_2^2}{2}}\left\|\begin{matrix}
            0.5  & 0.5 \\ 
            0.5 &  -0.5
        \end{matrix}\right\| \\
        & = \frac{1}{4\sqrt{2\pi}} (y_1-y_2) e^{-\frac{y_1^2+y_2^2}{4}}, y_1\geqslant y_2
    \end{aligned}
\end{equation}

Symmetrically, when $Y_1 \leqslant Y_2$,

\begin{equation}
    f_{Y_1,Y_2}(y_1, y_2) = \frac{1}{4\sqrt{2\pi}} (y_2-y_1) e^{-\frac{y_1^2+y_2^2}{4}}, y_1\leqslant y_2
\end{equation}


So, the joint pdf of $Y_1$ and $Y_2$ is,

\begin{equation}
    f_{Y_1,Y_2}(y_1, y_2) \propto \left|y_1-y_2\right| e^{-\frac{y_1^2+y_2^2}{4}}
\end{equation}

By $\int_{\mathbb{R}^2}  f_{Y_1,Y_2}(y_1, y_2) dy_1dy_2= 1 $

\begin{equation}
    f_{Y_1,Y_2}(y_1, y_2) =  \frac{1}{8\sqrt{2\pi}}  \left|y_1-y_2\right| e^{-\frac{y_1^2+y_2^2}{4}}
\end{equation}

\end{document}


