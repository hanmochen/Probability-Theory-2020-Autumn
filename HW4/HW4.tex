\documentclass{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{parskip}
\usepackage{graphicx}
\usepackage{hyperref}
% Margins
\usepackage[top=2.5cm, left=3cm, right=3cm, bottom=4.0cm]{geometry}
% Colour table cells
\usepackage[table]{xcolor}

% Get larger line spacing in table
\newcommand{\tablespace}{\\[1.25mm]}
\newcommand\Tstrut{\rule{0pt}{2.6ex}}         % = `top' strut
\newcommand\tstrut{\rule{0pt}{2.0ex}}         % = `top' strut
\newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}}   % = `bottom' strut

%%%%%%%%%%%%%%%%%
%     Title     %
%%%%%%%%%%%%%%%%%
\title{Exercise 4 \\ Probability Theory 2020 Autumn}
\author{Hanmo Chen \\ Student ID 2020214276}
\date{\today}

\begin{document}
\maketitle
\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%
%   Problem 1   %
%%%%%%%%%%%%%%%%%
\section{Problem 1}
\subsection{(i)}

Claim that $M_X(s) = \infty$ for all $s\neq 0$. The proof is as below.

\begin{equation}
    M_X(s) = \mathbb{E}[e^{sx}] = \int_{-\infty}^{\infty} \frac{1}{\pi} \frac{t}{t^2+x^2} e^{sx} dx = \int_{-\infty}^{\infty} \frac{1}{\pi} \frac{t}{t^2+x^2} e^{-sx} dx
\end{equation}

So $M_X(s) = M_X(-s)$, we just need to prove $M_X(s) = \infty$ for $s>0$. 

Notice that for $x>0$, $e^x > \frac {x^3}{6}$.

\begin{equation}
    \begin{aligned}
        M_X(s) & = \int_{-\infty}^{\infty} \frac{1}{\pi} \frac{t}{t^2+x^2} e^{sx} dx > \int_{0}^{\infty} \frac{1}{\pi} \frac{t}{t^2+x^2} e^{sx} dx \\
        & > \int_{0}^{\infty} \frac{1}{6\pi} \frac{ts^3 x^3}{t^2+x^2} dx =  \int_{0}^{\infty} \frac{t^3s^3}{6\pi} \frac{x^3}{1+x^2} dx \\
        & = \frac{t^3s^3}{12\pi} (x^2-\ln(1+x^2)) \big|_{0}^{\infty} = \infty
    \end{aligned}
\end{equation}

So $M_X(s) = \infty$ for all $s\neq 0$. And it is trivial that $M_X(0) = 1$.


\begin{equation}
    M_X(s) = \left\{  \begin{aligned}
        &1,\quad s =0\\
        &\infty, \quad s\neq 0
    \end{aligned}
        \right.
\end{equation}

\subsection{(ii)}

Yes. An example is given as the symmetrized lognormal distribution.

For example, $Z \sim N(0,1)$ and $Y = e^Z \sim \text{lognormal}(0,1)$ and define $X$ as 

\begin{equation}
    X = \left\{  \begin{aligned}
        &Y,\quad \text{with probability } \frac{1}{2} \\
        &-Y, \quad \text{with probability } \frac{1}{2}
    \end{aligned}\right.
\end{equation}

It is easy to verify that 

\begin{equation}
    f_Y(y) = f_Z(\ln y ) \frac {1}{y} = \frac{1}{\sqrt{2\pi} y} e^{-\frac{1}{2} (\ln y)^2}
\end{equation}

And the $n$-th moment of $Y$ is 

\begin{equation}
    \begin{aligned}
        \mathbb{E}[Y^n] & = \int_{0}^{\infty}  \frac{1}{\sqrt{2\pi} y} e^{-\frac{1}{2} (\ln y)^2} * y^n dy = \int_{0}^{\infty} \frac{1}{\sqrt{2\pi} y} \exp \left( -\frac{1}{2 }(\ln y)^2 + n\ln y\right) dy \\
        & = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} z^2 + \frac{n^2}{2}\right) dz \quad \left(z=\ln y - n\right) \\
        & = e^{\frac{n^2}{2}}
    \end{aligned}
\end{equation}

And 

\begin{equation}
    f_X(x) = \frac{1}{2\sqrt{2\pi} |x|} e^{-\frac{1}{2} (\ln |x|)^2}
\end{equation}


Then we will show that all the moments of $X$ are finite. For all odd integers $k$,

\begin{equation}
    \mathbb{E}[X^k] = \mathbb{E}[X^k_+] - \mathbb{E}[X^k_-] = \frac{1}{2} \mathbb{E}[Y^k] -  \frac{1}{2} \mathbb{E}[Y^k] = 0
\end{equation}

For all even integers $k$,

\begin{equation}
    \mathbb{E}[X^k] = \frac{1}{2} \mathbb{E}[Y^k] +  \frac{1}{2} \mathbb{E}[Y^k] = e^{\frac{n^2}{2}}
\end{equation}

So $\mathbb{E}[X^k] < \infty$ for all integers $k\geqslant 1$. But for the moment generating function $M_X(s)$, assuming $s>0$,

\begin{equation}
    \begin{aligned}
        M_X(s) & = \mathbb{E}[e^{sx}] = \int_{-\infty}^{\infty}  \frac{1}{2\sqrt{2\pi} |x|} e^{-\frac{1}{2} (\ln |x|)^2} e^{sx} dx > \int_{0}^{\infty}  \frac{1}{2\sqrt{2\pi} x} e^{-\frac{1}{2} (\ln x)^2} e^{sx} dx  \\
        & = \int_{-\infty}^{\infty}  \frac{1}{2\sqrt{2\pi} } e^{-\frac{1}{2} z^2} e^{se^z} dz > \int_{0}^{\infty} \frac{1}{2\sqrt{2\pi} } e^{\frac{s}{6} z^3-\frac{1}{2} z^2} \\
        & >  \int_{\frac{6}{s}}^{\infty} \frac{1}{2\sqrt{2\pi} } e^{\frac{1}{2} z^2} = \infty \quad (\frac{s}{6} z^3 > z^2 \text{ when } z>\frac{6}{s}) 
    \end{aligned}
\end{equation}

$M_X(s) = \infty$ for $s>0$ and $M_X(s) = M_X(-s)$, so  $M_X(s) = \infty$ for $s\neq 0$

\section{Problem 2}

\subsection{(a)}

Notice that the conclusion in (a) is a weaker version of the conclusion from (b) because $X^k \leqslant X^k e^{sX}$ when $s>0$, we will just prove (b).

\subsection{(b)}

If $s<0$, $X^k e^{sX} \leqslant X^k \leqslant X^k e^{s'X},s'>0$, so we will just prove the case when $s>0$.

Because $X$ is nonnegative,

\begin{equation}
    e^{ax} = e^{(a-s)x}  > \frac{(a-s)^k x^k}{k!}e^{sx}
\end{equation}

So $x^ke^{sx} < e^{ax} \frac {k!}{(a-s)^k}$,

\begin{equation}
    \mathbb{E}[X^ke^{sX}] \leqslant \mathbb{E}[\frac {k!}{(a-s)^k} e^{aX}] = \frac {k!}{(a-s)^k} M_X(a) < \infty
\end{equation}

\subsection{(c)}

The inequality holds true only when $h>0$. The following proof is given under the condition $h>0$.

It's obvious when $X=0$. And for $X>0$, it is equivalent to $\frac {e^{hX}-1}{hX} \leqslant e^{hX}$, denote $b$ as $hX$ and $g(x)$ as $e^X$, using Lagrange's mean value theorm,

\begin{equation}
    \frac{e^{hX}-1}{hX} = \frac{g(b)-g(0)}{b-0} = g'(c) \leqslant g'(b) = e^{hX}, c \in (0,b)
\end{equation}

\subsection{(d)}

Using L'Hospital's Rule, $X = \lim\limits_{h\to 0} \frac{e^{hX}-1}{h}$. 
Suppose $\{h_n\}$ is a sequence of nonnegative numbers and $\lim\limits_{n\to \infty} h_n = 0$, so $h_n<\frac{a}{2}$ for sufficiently large $n$. And define $X_n = \frac {e^{h_n X}-1}{h_n}$.

Using the conclusion from (c)，

\begin{equation}
    0<X_n\leqslant  Xe^{h_nX} < Xe^{\frac{a}{2}X}
\end{equation}  

And $\mathbb{E}[Xe^{\frac{a}{2}X}] < \infty$, so according to the Dominant Convergence Theorm,

\begin{equation}
    \mathbb{E}[X] = \mathbb{E}[\lim_{h\downarrow 0 } \frac{e^{hX}-1}{h}] = \lim_{h\downarrow 0 } \mathbb{E}[ \frac{e^{hX}-1}{h}] = \lim_{h\downarrow 0 } \frac{\mathbb{E}[e^{hX}]-1}{h}
\end{equation}

\section{Problem 3}

Let $Z= \frac{X}{\sigma} \sim N(0,1)$ and $z  = \frac{x}{\sigma}$. So $xe^{x^2/(2\sigma^2)} P(X\geqslant x) = \sigma z e^{z^2/2} P(Z\geqslant z)$.

Define the Mills Ratio as

\begin{equation}
    R(x) = \frac{1-\Phi(x)}{\phi(x)}  =  e^{\frac{x^2}{2}} \int_{x}^{\infty} e^{-\frac{t^2}{2}} dt,x>0
\end{equation}

We will prove that $\frac{x}{x^+1} <  R(x) < \frac{1}{x}, x>0$, which is first given by Gorden(1941)\footnote{Gordon, RD: Values of Mills’ ratio of area to bounding ordinate and of the normal probability integral for large values of the argument. Ann. Math. Stat. 12, 364-366 (1941)}. The following proof is based on the blog\footnote{\url{https://bowaggoner.com/blog/2018/03-17-gaussian-tails/index.html}}. Denote $\phi(t) = \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}$, then $\phi'(t) = -t \phi(t)$, 

Let $h_1(t) = -\frac{1}{t} \phi(t)$,

\begin{equation}
    \frac{d h_1(t)}{dt} = (\frac{1}{t^2}+1) \phi(t) 
\end{equation}

\begin{equation}
    \begin{aligned}
        1-\Phi(x) = \int_{x}^{\infty} \phi(t) dt < &\int_{x}^{\infty} (1+\frac{1}{t^2})\phi(t) dt \\
        & = h_1(t)\big|_{x}^{\infty} = \frac{\phi(x)}{x}
    \end{aligned}
\end{equation}

So $R(x) = \frac{1-\Phi(x)}{\phi(x)} < \frac{1}{x}$.

In the same way, let $h_2(t) = -\frac{t}{1+t^2} \phi(t)$,

\begin{equation}
    \frac{d h_2(t)}{dt} = \frac{x^4+2x^2-1}{(x^2+1)^2} \phi(t)  = (1-\frac{2}{(x^2+1)^2}) \phi(t)
\end{equation}

\begin{equation}
    \begin{aligned}
        1-\Phi(x) = \int_{x}^{\infty} \phi(t) dt > & \int_{x}^{\infty} ( (1-\frac{2}{(x^2+1)^2}) \phi(t)dt \\
        & = h_2(t)\big|_{x}^{\infty} = \frac{x}{x^2+1}\phi(x)
    \end{aligned}
\end{equation}

So $\frac{x}{x^+1}<R(x) < \frac{1}{x}$. Thus $\lim_{x\to \infty} xR(x) = 1$.

\begin{equation}
    \lim_{x\to\infty} xe^{x^2/(2\sigma^2)} P(X\geqslant x) = \lim_{z\to\infty} \sigma z e^{z^2/2} P(Z\geqslant z) = \sqrt{2\pi} \sigma \lim_{x\to \infty} xR(x) = \sqrt{2\pi} \sigma
\end{equation}

\section{Problem 4}

\subsection{(i)}

Suppose the pdf of $X_1,X_2,\cdots,X_n$ is $f_X(x)$ and the cdf is $F_X(x)$. And $P(\min(X_1,X_2,\cdots,X_n)= X_1) = P(X_1\leqslant \min (X_2,\cdots,X_n))$/. Define $Y = \min (X_2,\cdots,X_n)$, 

\begin{equation}
    F_Y(y) = P(\min (X_2,\cdots,X_n) \leqslant y) = 1- \prod_{i=2}^n P(X_i > y ) = 1-(1-F_X(y))^{n-1}
\end{equation}

Because $X_1$ and $X_2,\cdots,X_n$ are independent, $X_1$ and $Y = g(X_2,\cdots,X_n)$ are also independent.

\begin{equation}
    \begin{aligned}
        P(X_1\leqslant Y) & = \int_{\infty}^{\infty}f_X(x) P(Y\geqslant x) dx =  \int_{\infty}^{\infty}f_X(x) (1-F_X(x))^{n-1}  dx \\
        & = \int_{0}^{1}(1-F_X(y))^{n-1} d(1-F_X(x)) = \frac{1}{n}
    \end{aligned}
\end{equation}

So $P(\min(X_1,X_2,\cdots,X_n)= X_1) = \frac{1}{n}$.

\subsection{(ii)}

\begin{equation}
    P(\min(X_1,X_2,\cdots,X_n)= X_1) = P(X_1 =0) + P(X_1=1,X_2 = 1,\cdots,X_n = 1) = 1-p+p^n
\end{equation}

\subsection{(iii)}

$P(\min(X_1,X_2,\cdots,X_n)= X_1) = P(X_1\leqslant \min (X_2,\cdots,X_n))$. Define $Y = \min (X_2,\cdots,X_n)$, 

\begin{equation}
    F_Y(y) = P(\min (X_2,\cdots,X_n) \leqslant y) = 1- \prod_{i=2}^n P(X_i > y ) = 1- \exp(-\sum_{i=2}^n\lambda_i y),x\geqslant 0
\end{equation}

So $Y\sim Exponential(\sum_{i=2}^n\lambda_i)$ and $Y,X_1$ are independent.

\begin{equation}
    \begin{aligned}
        P(X_1\leqslant Y) & = \int_{0}^{\infty}f_X(x) P(Y\geqslant x) dx =  \int_{0}^{\infty}\lambda_1 \exp(-\lambda_1 x) \exp(-\sum_{i=2}^n\lambda_i x)  dx \\
        & = \frac{\lambda_1}{ \sum_{i=1}^n\lambda_i}
    \end{aligned}
\end{equation}

So $P(\min(X_1,X_2,\cdots,X_n)= X_1) = \frac{\lambda_1}{ \sum_{i=1}^n\lambda_i}$.

\end{document}


