\documentclass{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{parskip}
\usepackage{graphicx}
\usepackage{hyperref}
% Margins
\usepackage[top=2.5cm, left=3cm, right=3cm, bottom=4.0cm]{geometry}
% Colour table cells
\usepackage[table]{xcolor}

% Get larger line spacing in table
\newcommand{\tablespace}{\\[1.25mm]}
\newcommand\Tstrut{\rule{0pt}{2.6ex}}         % = `top' strut
\newcommand\tstrut{\rule{0pt}{2.0ex}}         % = `top' strut
\newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}}   % = `bottom' strut

%%%%%%%%%%%%%%%%%
%     Title     %
%%%%%%%%%%%%%%%%%
\title{Exercise 4 \\ Probability Theory 2020 Autumn}
\author{Hanmo Chen \\ Student ID 2020214276}
\date{\today}

\begin{document}
\maketitle
\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%
%   Problem 1   %
%%%%%%%%%%%%%%%%%
\section{Problem 1}
\subsection{(i)}

Claim that $M_X(s) = \infty$ for all $s\neq 0$. The proof is as below.

\begin{equation}
    M_X(s) = \mathbb{E}[e^{sx}] = \int_{-\infty}^{\infty} \frac{1}{\pi} \frac{t}{t^2+x^2} e^{sx} dx = \int_{-\infty}^{\infty} \frac{1}{\pi} \frac{t}{t^2+x^2} e^{-sx} dx
\end{equation}

So $M_X(s) = M_X(-s)$, we just need to prove $M_X(s) = \infty$ for $s>0$. 

Notice that for $x>0$, $e^x > \frac {x^3}{6}$.

\begin{equation}
    \begin{aligned}
        M_X(s) & = \int_{-\infty}^{\infty} \frac{1}{\pi} \frac{t}{t^2+x^2} e^{sx} dx > \int_{0}^{\infty} \frac{1}{\pi} \frac{t}{t^2+x^2} e^{sx} dx \\
        & > \int_{0}^{\infty} \frac{1}{6\pi} \frac{ts^3 x^3}{t^2+x^2} dx =  \int_{0}^{\infty} \frac{t^3s^3}{6\pi} \frac{x^3}{1+x^2} dx \\
        & = \frac{t^3s^3}{12\pi} (x^2-\ln(1+x^2)) \big|_{0}^{\infty} = \infty
    \end{aligned}
\end{equation}

So $M_X(s) = \infty$ for all $s\neq 0$. And it is trivial that $M_X(0) = 1$.


\begin{equation}
    M_X(s) = \left\{  \begin{aligned}
        &1,\quad s =0\\
        &\infty, \quad s\neq 0
    \end{aligned}
        \right.
\end{equation}

\subsection{(ii)}

Yes. An example is given as the symmetrized lognormal distribution.

For example, $Z \sim N(0,1)$ and $Y = e^Z \sim \text{lognormal}(0,1)$ and define $X$ as 

\begin{equation}
    X = \left\{  \begin{aligned}
        &Y,\quad \text{with probability } \frac{1}{2} \\
        &-Y, \quad \text{with probability } \frac{1}{2}
    \end{aligned}\right.
\end{equation}

It is easy to verify that 

\begin{equation}
    f_Y(y) = f_Z(\ln y ) \frac {1}{y} = \frac{1}{\sqrt{2\pi} y} e^{-\frac{1}{2} (\ln y)^2}
\end{equation}

And the $n$-th moment of $Y$ is 

\begin{equation}
    \begin{aligned}
        \mathbb{E}[Y^n] & = \int_{0}^{\infty}  \frac{1}{\sqrt{2\pi} y} e^{-\frac{1}{2} (\ln y)^2} * y^n dy = \int_{0}^{\infty} \frac{1}{\sqrt{2\pi} y} \exp \left( -\frac{1}{2 }(\ln y)^2 + n\ln y\right) dy \\
        & = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} z^2 + \frac{n^2}{2}\right) dz \quad \left(z=\ln y - n\right) \\
        & = e^{\frac{n^2}{2}}
    \end{aligned}
\end{equation}

And 

\begin{equation}
    f_X(x) = \frac{1}{2\sqrt{2\pi} |x|} e^{-\frac{1}{2} (\ln |x|)^2}
\end{equation}


Then we will show that all the moments of $X$ are finite. For all odd integers $k$,

\begin{equation}
    \mathbb{E}[X^k] = \mathbb{E}[X^k_+] - \mathbb{E}[X^k_-] = \frac{1}{2} \mathbb{E}[Y^k] -  \frac{1}{2} \mathbb{E}[Y^k] = 0
\end{equation}

For all even integers $k$,

\begin{equation}
    \mathbb{E}[X^k] = \frac{1}{2} \mathbb{E}[Y^k] +  \frac{1}{2} \mathbb{E}[Y^k] = e^{\frac{n^2}{2}}
\end{equation}

So $\mathbb{E}[X^k] < \infty$ for all integers $k\geqslant 1$. But for the moment generating function $M_X(s)$, assuming $s>0$,

\begin{equation}
    \begin{aligned}
        M_X(s) & = \mathbb{E}[e^{sx}] = \int_{-\infty}^{\infty}  \frac{1}{2\sqrt{2\pi} |x|} e^{-\frac{1}{2} (\ln |x|)^2} e^{sx} dx > \int_{0}^{\infty}  \frac{1}{2\sqrt{2\pi} x} e^{-\frac{1}{2} (\ln x)^2} e^{sx} dx  \\
        & = \int_{-\infty}^{\infty}  \frac{1}{2\sqrt{2\pi} } e^{-\frac{1}{2} z^2} e^{se^z} dz > \int_{0}^{\infty} \frac{1}{2\sqrt{2\pi} } e^{\frac{s}{6} z^3-\frac{1}{2} z^2} \\
        & >  \int_{\frac{6}{s}}^{\infty} \frac{1}{2\sqrt{2\pi} } e^{\frac{1}{2} z^2} = \infty \quad (\frac{s}{6} z^3 > z^2 \text{ when } z>\frac{6}{s}) 
    \end{aligned}
\end{equation}

$M_X(s) = \infty$ for $s>0$ and $M_X(s) = M_X(-s)$, so  $M_X(s) = \infty$ for $s\neq 0$

\section{Problem 2}

\subsection{(a)}

Notice that the conclusion in (a) is a weaker version of the conclusion from (b) because $X^k \leqslant X^k e^{sX}$ when $s>0$, we will just prove (b).

\subsection{(b)}

If $s<0$, $X^k e^{sX} \leqslant X^k \leqslant X^k e^{s'X},s'>0$, so we will just prove the case when $s>0$.

Because $X$ is nonnegative,

\begin{equation}
    e^{ax} = e^{(a-s)x}  > \frac{(a-s)^k x^k}{k!}e^{sx}
\end{equation}

So $x^ke^{sx} < e^{ax} \frac {k!}{(a-s)^k}$,

\begin{equation}
    \mathbb{E}[X^ke^{sX}] \leqslant \mathbb{E}[\frac {k!}{(a-s)^k} e^{aX}] = \frac {k!}{(a-s)^k} M_X(a) < \infty
\end{equation}

\subsection{(c)}

The inequality holds true only when $h>0$. The following proof is given under the condition $h>0$.

It's obvious when $X=0$. And for $X>0$, it is equivalent to $\frac {e^{hX}-1}{hX} \leqslant e^{hX}$, denote $b$ as $hX$ and $g(x)$ as $e^X$, using Lagrange's mean value theorm,

\begin{equation}
    \frac{e^{hX}-1}{hX} = \frac{g(b)-g(0)}{b-0} = g'(c) \leqslant g'(b) = e^{hX}, c \in (0,b)
\end{equation}

\subsection{(d)}

Using L'Hospital's Rule, $X = \lim\limits_{h\to 0} \frac{e^{hX}-1}{h}$. 
Suppose $\{h_n\}$ is a sequence of nonnegative numbers and $\lim\limits_{n\to \infty} h_n = 0$, so $h_n<\frac{a}{2}$ for sufficiently large $n$. And define $X_n = \frac {e^{h_n X}-1}{h_n}$.

Using the conclusion from (c)，

\begin{equation}
    0<X_n\leqslant  Xe^{h_nX} < Xe^{\frac{a}{2}X}
\end{equation}  

And $\mathbb{E}[Xe^{\frac{a}{2}X}] < \infty$, so according to the Dominant Convergence Theorm,

\begin{equation}
    \mathbb{E}[X] = \mathbb{E}[\lim_{h\downarrow 0 } \frac{e^{hX}-1}{h}] = \lim_{h\downarrow 0 } \mathbb{E}[ \frac{e^{hX}-1}{h}] = \lim_{h\downarrow 0 } \frac{\mathbb{E}[e^{hX}]-1}{h}
\end{equation}

\section{Problem 3}

Let $Z= \frac{X}{\sigma} \sim N(0,1)$ and $z  = \frac{x}{\sigma}$. So $xe^{x^2/(2\sigma^2)} P(X\geqslant x) = \sigma z e^{z^2/2} P(Z\geqslant z)$.

Define the Mills Ratio as

\begin{equation}
    R(x) = \frac{1-\Phi(x)}{\phi(x)}  =  e^{\frac{x^2}{2}} \int_{x}^{\infty} e^{-\frac{t^2}{2}} dt,x>0
\end{equation}

We will prove that $\frac{x}{x^+1} <  R(x) < \frac{1}{x}, x>0$, which is first given by Gorden(1941)\footnote{Gordon, RD: Values of Mills’ ratio of area to bounding ordinate and of the normal probability integral for large values of the argument. Ann. Math. Stat. 12, 364-366 (1941)}. The following proof is based on the blog\footnote{\url{https://bowaggoner.com/blog/2018/03-17-gaussian-tails/index.html}}. Denote $\phi(t) = \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}$, then $\phi'(t) = -t \phi(t)$, 

Let $h_1(t) = -\frac{1}{t} \phi(t)$,

\begin{equation}
    \frac{d h_1(t)}{dt} = (\frac{1}{t^2}+1) \phi(t) 
\end{equation}

\begin{equation}
    \begin{aligned}
        1-\Phi(x) = \int_{x}^{\infty} \phi(t) dt < &\int_{x}^{\infty} (1+\frac{1}{t^2})\phi(t) dt \\
        & = h_1(t)\big|_{x}^{\infty} = \frac{\phi(x)}{x}
    \end{aligned}
\end{equation}

So $R(x) = \frac{1-\Phi(x)}{\phi(x)} < \frac{1}{x}$.

In the same way, let $h_2(t) = -\frac{t}{1+t^2} \phi(t)$,

\begin{equation}
    \frac{d h_2(t)}{dt} = \frac{x^4+2x^2-1}{(x^2+1)^2} \phi(t)  = (1-\frac{2}{(x^2+1)^2}) \phi(t)
\end{equation}

\begin{equation}
    \begin{aligned}
        1-\Phi(x) = \int_{x}^{\infty} \phi(t) dt > & \int_{x}^{\infty} ( (1-\frac{2}{(x^2+1)^2}) \phi(t)dt \\
        & = h_2(t)\big|_{x}^{\infty} = \frac{x}{x^2+1}\phi(x)
    \end{aligned}
\end{equation}

So $\frac{x}{x^2+1}<R(x) < \frac{1}{x}$. Thus $\lim_{x\to \infty} xR(x) = 1$.

\begin{equation}
    \lim_{x\to\infty} xe^{x^2/(2\sigma^2)} P(X\geqslant x) = \lim_{z\to\infty} \sigma z e^{z^2/2} P(Z\geqslant z) = \frac{ \sigma}{\sqrt{2\pi}} \lim_{x\to \infty} xR(x) =  \frac{ \sigma}{\sqrt{2\pi}}
\end{equation}

\section{Problem 4}

\subsection{(i)}

Suppose the pdf of $X_1,X_2,\cdots,X_n$ is $f_X(x)$ and the cdf is $F_X(x)$. And $P(\min(X_1,X_2,\cdots,X_n)= X_1) = P(X_1\leqslant \min (X_2,\cdots,X_n))$/. Define $Y = \min (X_2,\cdots,X_n)$, 

\begin{equation}
    F_Y(y) = P(\min (X_2,\cdots,X_n) \leqslant y) = 1- \prod_{i=2}^n P(X_i > y ) = 1-(1-F_X(y))^{n-1}
\end{equation}

Because $X_1$ and $X_2,\cdots,X_n$ are independent, $X_1$ and $Y = g(X_2,\cdots,X_n)$ are also independent.

\begin{equation}
    \begin{aligned}
        P(X_1\leqslant Y) & = \int_{\infty}^{\infty}f_X(x) P(Y\geqslant x) dx =  \int_{\infty}^{\infty}f_X(x) (1-F_X(x))^{n-1}  dx \\
        & = \int_{0}^{1}(1-F_X(y))^{n-1} d(1-F_X(x)) = \frac{1}{n}
    \end{aligned}
\end{equation}

So $P(\min(X_1,X_2,\cdots,X_n)= X_1) = \frac{1}{n}$.

\subsection{(ii)}

\begin{equation}
    P(\min(X_1,X_2,\cdots,X_n)= X_1) = P(X_1 =0) + P(X_1=1,X_2 = 1,\cdots,X_n = 1) = 1-p+p^n
\end{equation}

\subsection{(iii)}

$P(\min(X_1,X_2,\cdots,X_n)= X_1) = P(X_1\leqslant \min (X_2,\cdots,X_n))$. Define $Y = \min (X_2,\cdots,X_n)$, 

\begin{equation}
    F_Y(y) = P(\min (X_2,\cdots,X_n) \leqslant y) = 1- \prod_{i=2}^n P(X_i > y ) = 1- \exp(-\sum_{i=2}^n\lambda_i y),x\geqslant 0
\end{equation}

So $Y\sim Exponential(\sum_{i=2}^n\lambda_i)$ and $Y,X_1$ are independent.

\begin{equation}
    \begin{aligned}
        P(X_1\leqslant Y) & = \int_{0}^{\infty}f_X(x) P(Y\geqslant x) dx =  \int_{0}^{\infty}\lambda_1 \exp(-\lambda_1 x) \exp(-\sum_{i=2}^n\lambda_i x)  dx \\
        & = \frac{\lambda_1}{ \sum_{i=1}^n\lambda_i}
    \end{aligned}
\end{equation}

So $P(\min(X_1,X_2,\cdots,X_n)= X_1) = \frac{\lambda_1}{ \sum_{i=1}^n\lambda_i}$.

\section{Problem 5}


Denote $\Phi(x)$ and $\phi(x)$ as the pdf and cdf of standard Gaussian variable. So 

\begin{equation}
    P(X\geq \beta \sqrt{\log(n)}) = 1- [\Phi(\beta \sqrt{\log(n)})]^n
\end{equation}

According to Problem 3, 

\begin{equation}
    1- \frac{1}{x}\phi(x)<\Phi(x) < 1- \frac{x}{x^2+1} \phi(x)
\end{equation}

So 


\begin{equation}
    \left(1-\frac{1}{\sqrt{2\pi \log(n)} \beta n^{\beta^2/2}}\right)^n < [\Phi(\beta \sqrt{\log(n)})]^n < \left(1-\frac{\beta \sqrt{\log (n)}}{\sqrt{2\pi} (\beta^2\log(n)+1) n^{\beta^2/2}}\right)^n
\end{equation}

Denote $L_n(\beta) = \left(1-\frac{1}{\sqrt{2\pi \log(n)} \beta n^{\beta^2/2}}\right)^n,U_n(\beta) = \left(1-\frac{\beta \sqrt{\log (n)}}{\sqrt{2\pi} (\beta^2\log(n)+1) n^{\beta^2/2}}\right)^n$.

If $\beta >\sqrt{2}$, $\beta^2/2 >1$,  $\lim\limits_{n\to\infty} L_n(\beta) = \lim\limits_{n\to\infty} Un(\beta) = 1$.  So $\lim\limits_{n\to\infty} P(X\geq \beta \sqrt{\log(n)}) = 0$

If $\beta <\sqrt{2}$, $\beta^2/2 <1$,  $\lim\limits_{n\to\infty} L_n(\beta) = \lim\limits_{n\to\infty} Un(\beta) = 0$.  So $\lim\limits_{n\to\infty} P(X\geq \beta \sqrt{\log(n)}) = 1$

So $\beta_0 = \sqrt{2}$. And when $\beta = \sqrt{2}$, using Bernoulli's inequality $(1+x)^n \geqslant 1+nx$ for $x\geqslant -1$,

$\lim\limits_{n\to\infty} L_n(\sqrt{2}) = \lim\limits_{n\to\infty} \left(1-\frac{1}{2\sqrt{\pi \log(n)} n}\right)^n \geqslant \lim\limits_{n\to\infty} 1-\frac{1}{2\sqrt{\pi \log(n)}} = 1$

Also we have $L_n(\sqrt{2}) \leqslant 1$ so  $\lim\limits_{n\to\infty} L_n(\sqrt{2}) = 1$

And $U_n(\beta) \leqslant 1$, so for $\beta= \sqrt{2},\lim\limits_{n\to\infty} P(X\geq \beta \sqrt{\log(n)}) = 1$

\section{Problem 6}

Denote $V_n$ as $Var(X_n)$, we have 

\begin{equation}
    X_n = \sum_{i=1}^{X_{n-1}} W_i
\end{equation}

with $\mathbb{E}[W_i] = \mu,\operatorname{Var}(W_i) = \sigma^2$.

First we need to use the Law of Total Variance 

\begin{equation}
    \operatorname{Var}(X)=\mathbb{E}[\operatorname{Var}(X \mid Y)]+\operatorname{Var}(\mathbb{E}[X \mid Y])
\end{equation}

The proof is given as below.

Because $\operatorname{Var} (X|Y) = \mathbb{E}[(X-\mathbb{E}[X|Y])^2 |Y] = \mathbb{E}[X^2|Y] - (\mathbb{E}[X|Y])^2$,

\begin{equation}
  \begin{aligned}
    \mathbb{E}[\operatorname{Var} (X|Y)]& = \mathbb{E}[ \mathbb{E}[X^2|Y] - (\mathbb{E}[X|Y])^2] \\& = \mathbb{E}[X^2] - \mathbb{E}[(\mathbb{E}[X|Y])^2]
  \end{aligned}
\end{equation}

Meanwhile,

\begin{equation}
  \begin{aligned}
    \operatorname{Var}(\mathbb{E}[X|Y]) & = \mathbb{E} [ (\mathbb{E}[X|Y])^2 ]- (\mathbb{E}[\mathbb{E}[X|Y]])^2 \\
    & = \mathbb{E} [ (\mathbb{E}[X|Y])^2 ]- (\mathbb{E}[X])^2 
  \end{aligned}
\end{equation}

Therefore,

\begin{equation}
\begin{aligned}
  \mathbb{E}[\operatorname{Var} (X|Y)] + \operatorname{Var}(\mathbb{E}[X|Y])  = &\mathbb{E}[x^2] - (\mathbb{E}[x])^2 \\ =&\operatorname{Var}(X)
\end{aligned}
\end{equation}


Using the Law of Total Variance, 

\begin{equation}
    \begin{aligned}
        \operatorname{Var}(X_n) = \mathbb{E}[\operatorname{Var} (X_{n}|X_{n-1})] + \operatorname{Var}(\mathbb{E}[X_n|X_{n-1}])
    \end{aligned}
\end{equation}

For the first item,

\begin{equation}
    \mathbb{E}\left[\operatorname{Var} \left(X_{n}|X_{n-1}\right)\right] = \mathbb{E}\left[\operatorname{Var} \left(\sum_{i=1}^{X_{n-1}} W_i|X_{n-1}\right)\right] = \mathbb{E} \left[X_{n-1} \sigma^2\right] = \sigma^2 \mathbb{E}[X_{n-1}]
\end{equation}

For the second item,

\begin{equation}
    \operatorname{Var}\left(\mathbb{E}\left[X_n|X_{n-1}\right]\right) = \operatorname{Var}\left(\mathbb{E}\left[\sum_{i=1}^{X_{n-1}} W_i|X_{n-1}\right]\right) =  \operatorname{Var}\left({X_{n-1}} \mu \right) = \mu^2 \operatorname{Var}(X_{n-1})
\end{equation}

Also we have 

\begin{equation}
    \mathbb{E}[X_n] = \mathbb{E} [\mathbb{E}X_n | X_{n-1}] = \mu \mathbb{E}[X_{n-1}] = \mu^n
\end{equation}


So 

\begin{equation}
    V_n  = \sigma^2 \mu^{n-1} + \mu^2 V_{n-1}
\end{equation}

And $V_1 = \operatorname{Var}(W_1) = \sigma^2$. If $\mu = 1$, $V_n = n\sigma^2$.

If $\mu \neq 1$,  

\begin{equation}
    \frac{V_n}{\mu^{2n}} = \frac{V_{n-1}}{\mu^{2n-2}} + \frac{\sigma^2}{\mu^{n+1}} = \sum_{i=1}^{n} \frac{\sigma^2}{\mu^{i+1}} = \frac{\sigma^2}{\mu^{n+1}} \frac{\mu^n-1}{\mu -1}
\end{equation}

So $\operatorname{Var}(X_n) = \sigma^2 \mu^{n-1}\frac{\mu^n-1}{\mu -1}$. 

In summary,

\begin{equation}
    \operatorname{Var}(X_n) = \left\{ 
        \begin{aligned}
            &n\sigma^2, &\quad \text{if } \mu = 1, \\
            &\sigma^2 \mu^{n-1}\frac{\mu^n-1}{\mu -1}, & \quad \text{if } \mu \neq 1.
        \end{aligned}
        \right.
\end{equation}


\end{document}


