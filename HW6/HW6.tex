\documentclass{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{parskip}
\usepackage{graphicx}
\usepackage{hyperref}
% Margins
\usepackage[top=2.5cm, left=3cm, right=3cm, bottom=4.0cm]{geometry}
% Colour table cells
\usepackage[table]{xcolor}

% Get larger line spacing in table
\newcommand{\tablespace}{\\[1.25mm]}
\newcommand\Tstrut{\rule{0pt}{2.6ex}}         % = `top' strut
\newcommand\tstrut{\rule{0pt}{2.0ex}}         % = `top' strut
\newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}}   % = `bottom' strut

%%%%%%%%%%%%%%%%%
%     Title     %
%%%%%%%%%%%%%%%%%
\title{Exercise 5 \\ Probability Theory 2020 Autumn}
\author{Hanmo Chen \\ 2020214276}
\date{\today}

\begin{document}
\maketitle

\section{Problem 1}

Yes. An example is given as below.

Let $X \sim N(0,1)$, denote $\alpha = \Phi^{-1}(0.75) = 0.674$. So $P(-\alpha \leqslant X \leqslant \alpha) = 0$. Define $Y$ as,

\begin{equation}
    Y = \left\{\begin{aligned}
        & X, \quad \text{if } -\alpha \leqslant X \leqslant \alpha \\
        & -X, \quad \text{if } X <-\alpha  \text{ or } X>\alpha
    \end{aligned}\right.
\end{equation}

And the cdf of $Y$ is,

\begin{equation}
    F_Y(y) = P(Y\leqslant y ) = \left\{\begin{aligned}
        & P(X\geqslant -y) = \Phi(y),&\quad  y\leqslant  - \alpha\\
        &  P(Y\leqslant - \alpha) + P(-\alpha < Y\leqslant  y) = \Phi(y), & \quad -\alpha \leqslant y \leqslant \alpha \\
        & P(Y\leqslant  \alpha) + P(\alpha < Y\leqslant  y) = \Phi(y),&\quad y > \alpha
    \end{aligned}\right.
\end{equation}

So $Y \sim N(0,1)$. But 

\begin{equation}
    X+Y = \left\{\begin{aligned}
        & 2X, \quad \text{if } -\alpha \leqslant X \leqslant \alpha \\
        & 0, \quad \text{if } X <-\alpha  \text{ or } X>\alpha
    \end{aligned}\right.
\end{equation}

So the distribution of $X + Y$ is not Gaussian distribution.

\section{Problem 2}


$X\sim N(0,1)$. Proof is given as below.

Let $X_1,X_2, \cdots,X_n,\cdots$ be i.i.d. random variables of the same distribution with $X$. 

Define $S_n = \sum_{i=1}^n X_i$. Let $T_{1} = \frac{X_1+X_2}{ \sqrt{2}} = \frac{S_2}{\sqrt{2}}$, $T_1' = \frac{X_3+X_4}{\sqrt{ 2}} = \frac{S_4 -S_2}{ \sqrt{2}}$, so $T_1,T_1'$ follows the same distribution with $X$. Let $T_2 = \frac{T_1+T_1'}{\sqrt{2}} = \frac{S_4}{\sqrt{4}}$ also follows the same distribution of $X$. And define $T_n = \frac{S_{2^n}}{\sqrt{2^n}}$, and its distribution is also the same distribution of $X$. 

According to CLT, 

\begin{equation}
    \frac{S_{2^n}}{\sqrt{2^n}} \overset{d.}{\to} N(0,1)
\end{equation}

So the distribution of $X$ is $N(0,1)$.

Note: another possible method is to consider the characteristic function $\phi_{X}(t)$, we can get an equation $\phi_{X}(t) = [\phi_{X}(\frac{t}{\sqrt{2}})]^2$. Also we have $\phi_{X}(0) =1,\phi_{X}'(0) =0,\phi_{X}''(0) =1 $. By solving the function equation we can get $\phi_X(t) = \exp({-\frac{t^2}{2}})$ and $X \sim N(0,1)$. 

\section{Problem 3}
\subsection{(i)}
Denote $-S$ as the exponent of the density function.

\begin{equation}
    \begin{aligned}
        S & = \frac{1}{2}\left(x_{1}^{2}+\sum_{i=1}^{2 n-2}\left(x_{i+1}-x_{i}\right)^{2}+x_{2 n-1}^{2}\right) \\
        & = \sum_{i=1}^{2n-1} x_i^2 - \sum_{i=1}^{2 n-2} x_{i+1}x_i\\
        & = \sum_{i=1}^{2n-2} A_i( x_i-B_i x_{i+1})^2 + A_{2n-1} x_{2n-1}^2
    \end{aligned}
\end{equation}

To find $A_i,B_i$, compare the coefficents, 

\begin{equation}
    \left\{\begin{aligned}
        & 2 A_i B_i = 1\\
        & A_i B_i^2 + A_{i+1} = 1\\
        & A_1 = 1 
    \end{aligned}\right.
\end{equation}

By induction we have $B_n = \frac{n}{n+1},A_n = \frac{n+1}{2n}$. 

And let $Y_i = \sqrt{2A_i} ( X_i-B_i X_{i+1}),i=1,2,\cdots,2n-2,Y_{2n-1} = \sqrt{2A_{2n-1}} X_{2n-1} $. 

\begin{equation}
    f_{Y_1,\cdot,Y_n}(y_1,y_n) =\left(\prod_{i=1}^{2n-1} \sqrt{2A_i} \right)^{-1}c_n \exp\left(-\frac{1}{2}\left(\sum_{i=1}^{2n-1} y_i^2\right)\right)
\end{equation}

Obviously $(Y_1,\cdots,Y_{2n-1})$ is a Gaussian random vector, so $(X_1,\cdots,X_{2n-1})$ as linear combinations of $(Y_1,\cdots,Y_{2n-1})$ is also a Gaussian vector.

\subsection{(ii)}


\begin{equation}
    \left(\prod_{i=1}^{2n-1} \sqrt{2A_i} \right)^{-1}c_n  = (\sqrt{2\pi})^{-(2n-1)}
\end{equation}

So $c_n = \frac{\sqrt{2n}}{(\sqrt{2\pi})^{2n-1}}$

\subsection{(iii)}

To find $\operatorname{Var}(X_n)$ we need to find the inverse transform $X= M^{-1}Y$. 
However, since $Y_i$ are independent standard Gaussian variables, and $X_i $ are just linear combinations of $Y_i,Y_{i+1},\cdots, Y_{2n-1}$, so $X_{i+1}$ and $Y_i$ are independent.

By $X_i = \sqrt{\frac{i}{i+1}} Y_i + \frac{i}{i+1} X_{i+1}$,
\begin{equation}
    \operatorname{Var}(X_i) = \frac{i^2}{(i+1)^2}  \operatorname{Var}(X_{i+1}) + \frac{i}{i+1}
\end{equation}

And $\operatorname{Var}(X_{2n-1})  = \frac{2n-1}{2n}$. By induction,

\begin{equation}
    \operatorname{Var}(X_{i})  = \frac{(2n-i)i}{2n}
\end{equation}

So $\operatorname{Var}(X_{n}) = \frac{n}{2}$.

Note: another tricky method. Notice that $X_i$ are symmetric about $n$, so $\operatorname{Var}(X_{n-1}) = \operatorname{Var}(X_{n+1})$.

And by letting $i=n,n-1$ in (9),

\begin{equation}
    \left\{\begin{aligned}
        & \operatorname{Var}(X_n) = \frac{n^2}{(n+1)^2}  \operatorname{Var}(X_{n+1}) + \frac{n}{n+1} \\
        & \operatorname{Var}(X_{n-1}) = \frac{(n-1)^2}{n^2}  \operatorname{Var}(X_{n}) + \frac{n-1}{n}
    \end{aligned}
        \right.
\end{equation}

Solving the equation, we also get $\operatorname{Var}(X_{n}) = \frac{n}{2}$.


\section{Problem 4}

For Cauchy random variable $f_X(x) = \frac{1}{\pi} \frac{1}{x^2 + 1}$, the characteristic function is 

\begin{equation}
    \phi_X(t) = e^{-|t|}
\end{equation}

For $\frac{S_n}{n^k}$, the characteristic function is 

\begin{equation}
    \phi_{k}(t) = \left[\phi_X\left(\frac{t}{n^k}\right)\right]^n = e^{-\frac{|t|}{n^{k-1}}}
\end{equation}

\subsection{(i)}

When $k=1$, $\phi_{1}(t) = e^{-|t|}$. So $\frac{S_n}{n^k}$ is also a Cauchy random variable and converges in distribution.

\subsection{(ii)}

When $k=2$, $\phi_{2}(t) = e^{-\frac{|t|}{n}}$.

\begin{equation}
    \lim_{n\to \infty} \phi_{2}(t) = \lim_{n\to \infty}e^{-\frac{|t|}{n}} = 1
\end{equation}

Using Fourier inverse transform, we know that the pdf of $\frac{S_n}{n^2}$ converges to the Dirac function $\delta(x)$.

\begin{equation}
    \frac{S_n}{n^2} \overset{d.}{\to} 0
\end{equation}

\subsection{(iii)}

When $k=\frac{1}{2}$, $\lim\limits_{n\to \infty} \phi_{0.5}(t) = \lim\limits_{n\to \infty} e^{-|t|\sqrt{n}}$ doesn't converge for $t\neq 0$. So $\frac{S_n}{\sqrt{n}}$ doesn't converge in distribution.

\section{Problem 5}

For $X_k$, $\phi_{X_k}(t) = \frac{e^{ikt} + e^{-ikt}}{2} = \cos (kt)$. So

\begin{equation}
    \phi_{\frac{S_n}{n^k}} (t) = \prod_{i=1}^n \cos(\frac{i}{n^k}t) 
\end{equation}

\subsection{(i)}

When $k=2$, $\lim\limits_{n\to \infty}\frac{i}{n^k}t = 0$ for $i=1,2,\cdots,n$.

Consider the taylor series of $\ln(\cos(x))= -\frac{x^2}{2}+ O(x^3)$, as $x\to 0$,

\begin{equation}
    \cos(x) \sim \exp \left({-\frac{x^2}{2} + O(x^3)}\right)
\end{equation}

\begin{equation}
    \begin{aligned}
        \lim_{n\to \infty}\phi_{\frac{S_n}{n^2}} (t) & =  \lim_{n\to \infty}\prod_{i=1}^n \exp \left({-\frac{i^2t^2}{2n^4} + O(\frac{i^3t^3}{n^3})}\right) \\
        & = \lim_{n\to \infty} \exp \left({-\frac{t^2}{2n^4}\sum_{i=1}^n i^2 + \sum_{i=1}^n i^3t^3 O(\frac{1}{n^6})}\right) \\
        & = \lim_{n\to \infty} \exp \left({-\frac{t^2(n+1)(2n+1)}{12n^3} +  O(\frac{1}{n^2})}\right) \\
        & = 1
    \end{aligned}
\end{equation}

So $S_n/n^2 \overset{d.}{\to} 0$

\subsection{(ii)}

Using the same method,

\begin{equation}
    \begin{aligned}
        \lim_{n\to \infty}\phi_{\frac{S_n}{n^{3/2}}} (t) & =  \lim_{n\to \infty}\prod_{i=1}^n \exp \left({-\frac{i^2t^2}{2n^3} + O(\frac{i^3t^3}{n^{9/2}})}\right) \\
        & = \lim_{n\to \infty} \exp \left({-\frac{t^2}{2n^3}\sum_{i=1}^n i^2 + \sum_{i=1}^n i^3t^3 O(\frac{1}{n^{9/2}})}\right) \\
        & = \lim_{n\to \infty} \exp \left({-\frac{t^2(n+1)(2n+1)}{12n^2} +  O(\frac{1}{\sqrt{n}})}\right) \\
        & = \exp\left(-\frac{t^2}{6}\right)
    \end{aligned}
\end{equation}

So $S_n/n^{\frac{3}{2}} \overset{d.}{\to} N(0,\frac{1}{3})$ (which can also be concluded from Lyapunov CLT).


\subsection{(iii)}


As $S_n/n^{\frac{3}{2}} \overset{d.}{\to} N(0,\frac{1}{3})$, 

\begin{equation}
    \lim_{n \to \infty} P(\frac{S_n}{n} \leqslant x) = \lim_{n \to \infty} P(\frac{S_n}{n^{3/2}} \leqslant \frac{x}{\sqrt{n}})  = \frac{1}{2}
\end{equation}

\section{Problem 6}

\subsection{(i)}

According to SLLN, $ \frac{1}{n}\sum\limits_{i=1}^n \log (X_i) \overset{a.s.}{\to} E[\log(X_1)] = -\frac{1}{2} \log2$ and
$Y_n = \prod\limits_{i=1}^n X_i = \exp\left(\sum\limits_{i=1}^n \log (X_i)\right)$, so 

\begin{equation}
    P\left(\left\{w:\lim_{n\to \infty} \sqrt[n]{Y_n(w)} = \frac{1}{\sqrt{2}}\right\}\right) = P\left(\left\{w:\lim_{n\to \infty} \frac{1}{n}\sum_{i=1}^n \log (X_i(w))=-\frac{1}{2} \log2\right\}\right) = 1
\end{equation}

According to root criterion for convergence,

\begin{equation}
    P\left(\left\{w:\lim_{n\to \infty} S_n(w) \text{ converges} \right\}\right) = P\left(\left\{w:\lim_{n\to \infty} \sqrt[n]{Y_n(w)} = \frac{1}{\sqrt{2}}\right\}\right) = 1
\end{equation}

So $S_n$ converges almost surely. Denote the limit random variable as $S$.


Define $T_n^{(1)} = 1 + X_n,T_n^{(2)} = 1+ X_{n-1}T_n^{(1)},T_n^{(i+1)} = 1+ X_{n-i}T_n^{(i)}, i =1,2,\cdots,n-1$, by definition,

\begin{equation}
    T_n^{(n)} = 1+ X_1 + X_1X_2 + \cdots + X_1X_2\cdots X_n = S_n +1 
\end{equation}

Notice that $T_n^{(i)}$ are just functions of $(X_{n-i+1},\cdots,X_n)$, so $T_n^{(i)}$ is independent of $X_{n-i}$.

\begin{equation}
    E[T_n^{(i+1)}] = E[1+ X_{n-i}T_n^{(i)}] = 1 + E[X_{n-i}] E[T_n^{(i)}] =  1+ \frac{3}{4} E[T_n^{(i)}]
\end{equation}

And $E[T_n^{(1)}] = \frac{7}{4}$ so $E[T_n^{(n)}] = 4 - 3 \times (\frac{3}{4})^n, E[S_n] = E[T_n^{(n)}] - 1 = 3(1-(\frac{3}{4})^n)$,


\begin{equation}
    E[S] = \lim_{n \to \infty}  3(1-(\frac{3}{4})^n) = 3
\end{equation}


Because

\begin{equation}
    \begin{aligned}
        \operatorname{Var}(XY) & = E[X^2Y^2] - (E[XY])^2= E[X^2] E[Y^2] - (E[X]E[Y])^2 \\
        & = \operatorname{Var}(X)\operatorname{Var}(Y) + \operatorname{Var}(X) (E[Y])^2  +  \operatorname{Var}(Y) (E[X])^2 
    \end{aligned}
\end{equation}

\begin{equation}
    \operatorname{Var}(T_n^{(i+1)}) = \frac{5}{8} \operatorname{Var}(T_n^{(i)}) + \frac{1}{16} (4 - 3 \times (\frac{3}{4})^i)^2 
\end{equation}

And $ \operatorname{Var}(T_n^{(1)}) = \frac{1}{16}, \operatorname{Var}(T_n^{(n)}) = (\frac{5}{8})^n \sum\limits_{i=0}^{n-1}(\frac{8}{5})^{i+1} (1 -(\frac{3}{4})^{i+1})^2  $.

\begin{equation}
    \operatorname{Var}(S) = \lim_{n \to \infty}   =  \operatorname{Var}(T_n^{(n)}) = \frac{8}{3}
\end{equation}

Note: let $ n,i \to \infty$ in (24) and (27), we can get 

\begin{equation}
    \left\{
    \begin{aligned}
        & E[S] = 1 + \frac{3}{4} E[S] \\
        &\operatorname{Var}(S) = \frac{5}{8} \operatorname{Var}(S) + 1
    \end{aligned}
    \right.
\end{equation}

which also leads to $E[S] = 3, \operatorname{Var}(S) = \frac{8}{3}$

\subsection{(ii)}

Also we have $E[\log(X_1)] = -\frac{1}{2} \log2<0$, thus  

\begin{equation}
    P\left(\left\{w:\lim_{n\to \infty} S_n(w) \text{ converges} \right\}\right) = P\left(\left\{w:\lim_{n\to \infty} \sqrt[n]{Y_n(w)} = \frac{1}{\sqrt{2}}\right\}\right) = 1
\end{equation}

So $S_n$ converges almost surely. But $E[S_n]$ and $\operatorname(S_n)$ increases to $\infty$!

\end{document} 
