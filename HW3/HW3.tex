\documentclass{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{parskip}
\usepackage{graphicx}
\usepackage{hyperref}
% Margins
\usepackage[top=2.5cm, left=3cm, right=3cm, bottom=4.0cm]{geometry}
% Colour table cells
\usepackage[table]{xcolor}

% Get larger line spacing in table
\newcommand{\tablespace}{\\[1.25mm]}
\newcommand\Tstrut{\rule{0pt}{2.6ex}}         % = `top' strut
\newcommand\tstrut{\rule{0pt}{2.0ex}}         % = `top' strut
\newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}}   % = `bottom' strut

%%%%%%%%%%%%%%%%%
%     Title     %
%%%%%%%%%%%%%%%%%
\title{Exercise 3 \\ Probability Theory 2020 Autumn}
\author{Hanmo Chen \\ Student ID 2020214276}
\date{\today}

\begin{document}
\maketitle
\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%
%   Problem 1   %
%%%%%%%%%%%%%%%%%
\section{Problem 1}

\textbf{No}. A counterexample is given as below.

Let $\Omega$ be $(0,1)$ and the probability measure $\mathbb{P} = \lambda$. Define $X_n$ as

\begin{equation}
    X_n = -\frac{1}{nx}, \quad x \in (0,1), n\geqslant 1
\end{equation}

So $X_{n+1}(w) \geqslant X_n(w)$ for all $n \geqslant 1$ and  $w\in \Omega$.

And $X = \lim\limits_{n\to\infty} X_n = 0$. So $\operatorname{E}[X] = 0$ but $\operatorname{E}[X_n] = -\infty$, $\lim\limits_{n\to\infty} \operatorname{E}[X_n] \neq \operatorname{E}[X]$

\section{Problem 2}

\subsection{(a)}

Using $\int_{\mathbb{R}} f_X(x)dx =1$, we have

\begin{equation}
    \int_{\mathbb{R}} f_X(x)dx = \int_{-2}^2 c\sqrt{4-x^2}dx = 2\pi c = 1
\end{equation}

So $c = \frac{1}{2\pi}$

\subsection{(b)}

Because $f_X(-x) = f_X(x)$, for $k=1,3,\cdots,2n+1,\cdots,  \operatorname{E}[X^k] = 0$.

For $k=2n,n=1,2,\cdots$

\begin{equation}
    \begin{aligned}
        \operatorname{E}[X^k] & =  \int_{\mathbb{R}} x^kf_X(x)dx =  \frac 1 {2\pi} \int_{-2}^2 x^k\sqrt{4-x^2} dx \\ 
        & =  \frac {1}{\pi} \int_{0}^{2}x^k\sqrt{4-x^2} dx \quad(x=2\sin\theta) \\
        & = \frac {2^{k+2}}{\pi}\int_{0}^{\frac \pi {2}}{\sin^k\theta \cos^2\theta} d\theta \\
        & = \frac {2^{k+2}}{\pi}\int_{0}^{\frac \pi {2}} \left[\sin^k\theta  - \sin^{k+2}\theta \right] d\theta
    \end{aligned}
\end{equation}

Denote $I_n = \int_{0}^{\frac \pi {2}} \sin^n x dx$,

\begin{equation}
    \begin{aligned}
        I_{n} & =  \int_{0}^{\frac \pi {2}} \sin^n x dx =  \int_{0}^{\frac \pi {2}} \sin^{n-1} x d(-\cos x) \\
        & = -\cos x\sin^{n-1} x \big|_{0}^{\frac \pi {2}} + (n-1)  \int_{0}^{\frac \pi {2}} \sin^{n-2}x\cos^2 x dx \\
        & = (n-1)(I_{n-2}-I_n)
    \end{aligned}
\end{equation}

So $I_n = \frac{n-1}{n} I_{n-2}$ and $I_0 = \frac{\pi}{2},I_2 = \frac{\pi}{4}$, 

$I_{2n}= \frac{(2n-1)(2n-3)\cdots 1}{(2n)(2n-2)\cdots 2} \frac {\pi} {2} = \frac {(2n)!} {(n!)^2 } \frac{\pi}{ 2^{2n+1}}$

So, For $k=2n,n=1,2,\cdots$


\begin{equation}
    \begin{aligned}
        \operatorname{E}[X^k] & =  \frac {2^{k+2}}{\pi}\int_{0}^{\frac \pi {2}} \left[\sin^k\theta  - \sin^{k+2}\theta \right] d\theta \\ 
        & = \frac {2^{k+2}}{\pi} \left(I_k - I_{k+2}\right) \\
        & = \frac {2^{k+2}}{\pi} \left( \frac {(k)!} {((k/2)!)^2 } \frac{\pi}{ 2^{k+1}} \frac {(k+2)!} {((k/2+1)!)^2 } \frac{\pi}{ 2^{k+3}} \right) \\
        & = \frac {2(k)!} {((k/2)!)^2 } - \frac {(k+2)!} {2((k/2+1)!)^2 } \\
        & = \frac{2}{k+2} \frac {(k)!} {((k/2)!)^2 }
    \end{aligned}
\end{equation}


In summary,

\begin{equation}
    \operatorname{E}[X^k] = \left\{
    \begin{aligned}
        &0, \quad k=2n-1,n=1,2,\cdots \\
        &\frac{2}{k+2} \frac {(k)!} {((k/2)!)^2 }, \quad k=2n,n=1,2,\cdots
    \end{aligned}
    \right.
\end{equation}

\section{Problem 3}

\textbf{Yes.}

To prove this, we define a series of random variables by replacing $X_i$ with $Y_i$ once a time. 

Define $Z_0 =X =  X_1+X_2+\cdots+X_n, Z_1 = Y_1 + X_2 + \cdots +X_n, Z_j= \sum_{i=1}^{j} Y_j + \sum_{i=J+1}^{n} X_i, Z_n = Y = Y_1+ Y_2 +\cdots+Y_n$

We want to prove $Z_{j}$ \textit{stochastically dominates} $Z_{j-1}$, that is,

$\forall k = 1,2,\cdots,n$, $P(Z_{j-1}\geqslant k) \geqslant P(Z_j\geqslant k)$

Notice that the only difference between $Z_{j-1}$ and $Z_j$ is the $j$-th item, $X_{j}$ or $Y_{j}$. 

Denote the sum of the left $n-1$ items as $Z'$, and $Z_{j-1} = Z'+X_{j},Z_{j}=Z'+Y_{j}$, also denote $P(X_{i} =1) = p_i \geqslant P(Y_{i} =1) = q_i$.

\begin{equation}
\begin{aligned}
    P(Z_{j-1}\geqslant k) &=  P(Z'+X_j \geqslant k) \\ &= P(Z'\geqslant k) + P(Z' = k-1,X_j=1) \\ &= P(Z'\geqslant k)+ P(Z' = k-1)p_j 
\end{aligned}
\end{equation}

In the same way, $ P(Z_{j}\geqslant k) = P(Z'\geqslant k)+ P(Z' = k-1)q_j $.

Since $p_j \geqslant q_j$, we prove that $\forall k = 1,2,\cdots,n,\quad P(Z_{j-1}\geqslant k) \geqslant P(Z_j\geqslant k)$


Therefore, $\forall k = 1,2,\cdots,n$

\begin{equation}
    P(X\geqslant k) = P(Z_{0}\geqslant k) \geqslant P(Z_{1}\geqslant k) \geqslant \cdots \geqslant P(Z_{n}\geqslant k) = P(Y \geqslant k)
\end{equation}

\section{Problem 4}

\subsection{(a)}

Suppose $V \sim \text{Exp}(\lambda)$, because $U,V$ are independent, $\operatorname{E}\left[\frac{V^2}{1+U}\right]= \operatorname{E}[V^2] \operatorname{E}\left[\frac{1}{1+U}\right]$.

\begin{equation}
    \operatorname{E}[V^2]  = \int_{0}^{\infty} \lambda x^2 e^{-\lambda x} dx = \int_{0}^{\infty} x^2 e^{- x} dx = -\frac{1}{\lambda^2} e^{-x} (2 + 2 x + x^2) \big|_{0}^{\infty} = \frac{2}{\lambda^2}
\end{equation}

\begin{equation}
    \operatorname{E}\left[\frac{1}{1+U}\right] = \int_{0}^{1} \frac {1}{1+x} dx = \ln 2
\end{equation}

So,

\begin{equation}
    \operatorname{E}\left[\frac{V^2}{1+U}\right]= \operatorname{E}[V^2] \operatorname{E}\left[\frac{1}{1+U}\right] = \frac{2\ln 2}{\lambda^2}
\end{equation}

\subsection{(b)}

\begin{equation}
    \begin{aligned}
        P(U\leqslant V) & = \int_{0}^1 P(U\leqslant v) f_V(v)dv \\
        & = \int_{0}^1 \int_{0}^x \lambda  e^{-\lambda y} dy dx \\
        & =  \int_{0}^1 (1-e^{-\lambda x})dx \\
        & = 1- \frac 1 {\lambda} (1-e^{-\lambda})
    \end{aligned}
\end{equation}

\subsection{(c)}

Because $U =\sqrt{Y}, V = \frac{Z}{\sqrt{Y}}$ 

\begin{equation}
    f_{Y,Z}(y, z)=f_{U,V}(u(y,z), v(y,z))\left|\frac{\partial(u,v)}{\partial(y,z)}\right|
\end{equation}

And the Jacobian matrix is,

\begin{equation}
    \begin{aligned}
        \left|J\right| = \left|\frac{\partial(u,v)}{\partial(y,z)}\right| = \left|\begin{matrix}
            \frac{1}{2\sqrt{Y}}&0\\-\frac{Z}{2Y\sqrt{Y}}&\frac{1}{\sqrt{Y}} 
        \end{matrix}\right| = \frac{1}{2Y}
    \end{aligned}
\end{equation}

So the joint pdf of $Y,Z$ is 

\begin{equation}
    \begin{aligned}
        f_{Y,Z}(y, z) = 1\times \lambda e^{-\lambda \frac z {\sqrt{y}}} \times \frac {1}{2y}
    \end{aligned}
\end{equation}

The support set of $Y,Z$ is $[0,1]\times [0,\infty)$.

\begin{equation}
    f_{Y,Z}(y, z) = \left\{\begin{aligned}
        &\frac {\lambda }{2y} e^{-\lambda \frac z {\sqrt{y}}}, \quad (y,z) \in [0,1]\times [0,\infty) \\
        &0,\quad \text{else}
    \end{aligned}\right.
\end{equation}

\section{Problem 5}

First $X_2+X_3$ follows the Gamma distribution $\Gamma(2,\frac{1}{\lambda})$ with $\frac{1}{\lambda}$ as the scale parameter, to derive this, let $Z = X_2+X_3$,

\begin{equation}
    \begin{aligned} f_{Z}(z) &=\int_{-\infty}^{\infty} f_{X_{2}}\left(x\right) f_{X_{3}}\left(z-x\right) d x 
    \\ &=\int_{0}^{z} \lambda e^{-\lambda x} \lambda  e^{-\lambda\left(z-x\right)} d x \\
    & =\int_{0}^{z} \lambda^2 e^{-\lambda z}  d x \\
& = \lambda^2 z e^{-\lambda z}\end{aligned}
\end{equation}


So

\begin{equation}
    \begin{aligned}
        P(X_1 > X_2+X_3) & = P(X_1 > Z) = \int_{0}^{\infty} f_{X_1}(x) P(Z<x)dx \\
        & = \int_{0}^{\infty}\left(\int_{0}^x \lambda^2 z e^{-\lambda z} dz \right) \lambda e^{-\lambda x} dx \\
        & =  \int_{0}^{\infty} \left(1-(1+\lambda x)e^{-\lambda x}\right) \lambda e^{-\lambda x} dx \\
        & = 1 - \int_{0}^{\infty} \lambda e^{-2\lambda x}dx - \int_{0}^{\infty} \lambda^2 x e^{-2\lambda x}dx \\
        & = 1 - \frac 1 2 - \frac{1}{4} = \frac 1 4 
    \end{aligned}
\end{equation}

\section{Problem 6}

The eigenvalues $Y_1,Y_2$ are the roots of the following equation,

\begin{equation}\label{eq1}
    (\lambda-X_1)(\lambda-X_2) = X_3^2
\end{equation}

\textbf{Part 1}

First, let $U=\frac {X_1+X_2} 2,V = \frac {X_1-X_2} 2$, we prove that $U,V \overset{i.i.d.}{\sim} N(0,1)$.

\begin{equation}
    \begin{aligned}
        f_{U,V}(u, v) & =f_{X_1,X_2}(x_1, x_2)\left|\frac{\partial(x_1, x_2)}{\partial(u, v)}\right| \\
        & = \frac {1} {4\pi } e^{-\frac {x_1^2+x_2^2}{4}} \left\|\begin{matrix}
            1  & 1 \\ 
            1 &  -1
        \end{matrix}\right\| \\
        &  = \frac {1}{2\pi} e^{-\frac{u^2 + v^2}{2}} \\
        & = \frac{1}{\sqrt{2\pi}} e^{-\frac{u^2}{2}} \times \frac{1}{\sqrt{2\pi}} e^{-\frac{v^2}{2}}
    \end{aligned}
\end{equation}

So $U,V$ are independent and follow the standard Gaussian distribution.

~\\

\textbf{Part 2}

By solving the equation \ref{eq1} we have 

\begin{equation}
    \left\{
    \begin{aligned}
        &Y_1 +Y_2 = X_1+X_2 = 2U \\
        &|Y_1 -Y_2| = 2\sqrt{\left(\frac{X_1-X_2}{2}\right)^2 + X_3^2} = 2\sqrt{V^2 + X_3^2}
    \end{aligned}
    \right.
\end{equation}

Denote $Z_1= \frac{Y_1+Y_2}{2}$, $Z_2 = \frac{|Y_1 -Y_2|}{2}$, so $Z_1 = U \sim N(0,1),Z_2 = \sqrt{V^2+X_3^2}$.

Because $X_1,X_2,X_3$ are independent, $U,V,X_3$ are also independent, $Z_1$ and $Z_2$ are independent. Because $V$ and $X_3$ are standard Gaussian random variables $,Z_2 = \sqrt{V^2+X_3^2} \sim \chi_{2}$ (also known as the Rayleigh distribution)

To derive $f_{Z_2}(z)$,

\begin{equation}
    \begin{aligned}
        F_{Z_2}(z) & = P(Z_2\leqslant z)  = P(X_3^2+V^2\leqslant z^2) = \iint_{x^2+y^2\leqslant z^2 } \frac{1}{2\pi} e^{-\frac{x^2+y^2}{2}} dxdy 
        \\ & = \frac 1{2\pi} \int_{0}^{2\pi} \int_{0}^{z} re^{-\frac{r^2}{2}} drd\theta \\
        & = \int_{0}^{z} re^{-\frac{r^2}{2}} dr \\
        & = 1- e^{-\frac {z^2}{2}}
    \end{aligned}
\end{equation}

And $f_{Z_2}(z) = F'_{Z_2}(z) = z e^{-\frac {z^2}{2}}, z\geqslant 0$

~\\

\textbf{Part 3}

Assume $Y_1 \geqslant Y_2$, 

\begin{equation}
    \begin{aligned}
        f_{Y_1,Y_2}(y_1, y_2) & =f_{Z_1,Z_2}(z_1, z_2)\left|\frac{\partial(z_1, z_2)}{\partial(y_1, y_2)}\right| \\
        & = \frac {1}{\sqrt{2\pi}} e^{-\frac{z_1^2}{2}} z_2 e^{-\frac {z_2^2}{2}}\left\|\begin{matrix}
            0.5  & 0.5 \\ 
            0.5 &  -0.5
        \end{matrix}\right\| \\
        & = \frac{1}{2\sqrt{2\pi}} (y_1-y_2) e^{-\frac{y_1^2+y_2^2}{4}}, y_1\geqslant y_2
    \end{aligned}
\end{equation}

Symmetrically, when $Y_1 \leqslant Y_2$,

\begin{equation}
    f_{Y_1,Y_2}(y_1, y_2) = \frac{1}{2\sqrt{2\pi}} (y_2-y_1) e^{-\frac{y_1^2+y_2^2}{4}}, y_1\leqslant y_2
\end{equation}


So, the joint pdf of $Y_1$ and $Y_2$ is,

\begin{equation}
    f_{Y_1,Y_2}(y_1, y_2) = \frac{1}{2\sqrt{2\pi}} \left|y_1-y_2\right| e^{-\frac{y_1^2+y_2^2}{4}}
\end{equation}


\end{document}


